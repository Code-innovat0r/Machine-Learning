{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f50a70a-5275-453b-8802-0a00fcfcd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f848fcab-9341-444c-81ad-5dc020170616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Access the registry, which is now a dictionary\n",
    "all_envs = gym.envs.registry\n",
    "\n",
    "# Extract the environment IDs (keys of the dictionary)\n",
    "env_ids = list(all_envs.keys())\n",
    "\n",
    "print(env_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc5811b9-3abd-4762-ab8e-b527f18b5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06e6e0b8-988e-4da6-bb0c-a78d23a26da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnAgent():\n",
    "    def __init__(self, n_states, n_act, e_grade=0.1, lr=0.1, gamma=0.9):\n",
    "        self.Q = np.zeros([n_states, n_act])\n",
    "        self.e_grade = e_grade\n",
    "        self.n_act = n_act\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def predict(self, state):\n",
    "        Q_list = self.Q[state, :]\n",
    "        \n",
    "\n",
    "        action = np.random.choice(np.flatnonzero(Q_list == Q_list.max()))\n",
    "        return action\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.uniform(0, 1) < self.e_grade:\n",
    "            action = np.random.choice(self.n_act)\n",
    "        else:\n",
    "            action = self.predict(state)\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        cur_Q = self.Q[state, action]\n",
    "        if done:\n",
    "            target_Q = reward\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * self.Q[next_state, :].max()\n",
    "        self.Q[state,action] += self.lr * (target_Q - cur_Q)\n",
    "\n",
    "def train_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def test_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def train(env, episodes = 500, e_grade=0.1, lr=0.1, gamma = 0.9):\n",
    "    agent = QLearnAgent(\n",
    "        n_states=env.observation_space.n,\n",
    "        n_act = env.action_space.n,\n",
    "        e_grade = e_grade,\n",
    "        lr=lr,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "    for epoch in range(episodes):\n",
    "        ep_reward = train_episode(env, agent)\n",
    "        print('Episode %s: reward= %.1f' % (epoch, ep_reward))\n",
    "        test_reward = test_episode(env, agent)\n",
    "        print('Test_reward= %.1f' % test_reward)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "212b3f8a-31c5-4dc3-8e01-0283dd0b5823",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(env)\n",
      "Cell \u001b[1;32mIn[51], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, episodes, e_grade, lr, gamma)\u001b[0m\n\u001b[0;32m     61\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearnAgent(\n\u001b[0;32m     62\u001b[0m     n_states\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mn,\n\u001b[0;32m     63\u001b[0m     n_act \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m---> 69\u001b[0m     ep_reward \u001b[38;5;241m=\u001b[39m train_episode(env, agent)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: reward= \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, ep_reward))\n\u001b[0;32m     71\u001b[0m     test_reward \u001b[38;5;241m=\u001b[39m test_episode(env, agent)\n",
      "Cell \u001b[1;32mIn[51], line 36\u001b[0m, in \u001b[0;36mtrain_episode\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m     37\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     38\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[51], line 20\u001b[0m, in \u001b[0;36mQLearnAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     18\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_act)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m, in \u001b[0;36mQLearnAgent.predict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 10\u001b[0m     Q_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[state, :])\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39mflatnonzero(Q_list \u001b[38;5;241m==\u001b[39m Q_list\u001b[38;5;241m.\u001b[39mmax()))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "    train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b7424-28bb-498e-9c29-03fffe54ecc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
